import numpy as np
import pickle
from multiprocessing import Pool
import time
import igraph
from IPython.display import Image, display
import os
import math


### input/output, reading data

def save_TSP(A, n, nodes_array, file_name):
    with open(file_name + '.matrix', "wb") as fh:
        pickle.dump((A, n, nodes_array), fh)


def load_TSP(file_name):
    with open(file_name, "rb") as fh:
        (A, n, nodes_array) = pickle.load(fh)
        return A, n, nodes_array


def save_LON(V, E, A, n, nodes_array, file_name):
    # graph: A - adjacency matrix with weights, n - dimensions
    # landscape: V - optima vertices, E - escape edges
    with open(file_name, "wb") as fh:
        pickle.dump((V, E, A, n, nodes_array), fh)


def load_LON(file_name):
    with open(file_name, "rb") as fh:
        (V, E, A, n, nodes_array) = pickle.load(fh)
        return V, E, A, n, nodes_array


def save_metrics(metrics_tuple, file_name):
    with open(file_name, "wb") as fh:
        pickle.dump(metrics_tuple, fh)


def load_metrics(file_name):
    with open(file_name, "rb") as fh:
        metrics_tuple = pickle.load(fh)
        return metrics_tuple


def parse_TSP_from_file(filename):
    with open(filename) as fh:
        data = fh.read().split("\n")  # lines of input file
    n = int(data[3].split()[1])  # dimensions (number of nodes)
    A = np.empty((n, n), dtype=int)  # adjacency matrix with weights

    for i in range(n):
        edges = map(int, data[i + 8].split())  # map(function, iterables), int(string, base)
        for j, v in enumerate(edges):
            A[i, j] = v

    return A, n


### fitness landscape

def generate_LON(A, runs_amount, termination_criterion, kick_strength):  # LON sampling (Algorithm 1 from paper [1])
    T = runs_amount  # number of Chained-LK runs
    I = termination_criterion  # termination criterion of run
    L = set()  # set of local optima
    E = {}  # escape edges

    run_result = {}
    iters = {}
    best_path = None
    best_path_length = math.inf

    pool = Pool()  # for parallel operations
    begin = time.time()
    for s_res, s, iters_, L_, E_ in pool.starmap(CLK, [(I, A, kick_strength) for t in range(T)]):  # starmap - map with multiple args
        L = L | L_  # set union  # TODO probable time losing place
        for k, v in E_.items():  # TODO plateaus?
            if k in E:
                E[k] += v
            else:
                E[k] = v

        if s_res in run_result:  # TODO test
            run_result[s_res] += 1
            iters[s_res] += iters_
        else:
            run_result[s_res] = 1
            iters[s_res] = iters_

        if s_res < best_path_length:
            best_path_length = s_res
            best_path = s  # one of best paths if multiple paths has the same length

    print("time", (time.time() - begin))
    return L, E, best_path_length, best_path, run_result[best_path_length], iters[best_path_length]/run_result[best_path_length]


def CLK(I, A, K):  # Iterated local search Chained-LK
    # intensification stage: Lin-Kernighan local search
    # diversification stage: double bridge (4-exchange perturbation)

    def hashable_path(path):
        index = np.where(path == 0)[0][0]
        return tuple(np.roll(path, -index))

    n = A.shape[0]  # dimensions (number of nodes)
    s = np.random.permutation(n)  # random visiting nodes order
    s_res = path_length(s, A, n)  # path length for order s
    L = {hashable_path(s)}  # set of local optima (estimation generated by sampling)
    E = {}  # escape edges (obtained by double bridge and LK)
    i = 0
    total_iters = 0
    while i < I:
        p = perturbate(s, K)  # K double-bridge operations
        p = intensify(p, A, n)
        p_res = path_length(p, A, n)
        i += 1
        if p_res < s_res:
            L.add(hashable_path(p))  # adding hashable object
            e = (hashable_path(s), hashable_path(p))
            if e in E:
                E[e] += 1
            else:
                E[e] = 1
            s = p
            s_res = p_res
            total_iters += i
            i = 0
    return s_res, s, total_iters, L, E


def path_length(p, A, n):
    s = 0.0
    for i in range(n):  # [0..n-1]
        s += A[p[i-1], p[i]]  # p[-1] - last element
    return s


def perturbate(nodes_order, K):  # K - kick strength (how many double-bridge operations are applied)
    def double_bridge(path):
        s = np.sort(np.random.choice(path.size, 3, replace=False))  # numpy.random.choice(int, sampleSize, copies?)
        return np.hstack([path[:s[0]], path[s[2]:], path[s[1]:s[2]], path[s[0]:s[1]]])

    nodes_order_ = nodes_order.copy()
    for i in range(K):
        nodes_order_ = double_bridge(nodes_order_)
    return nodes_order_


# LK  # TODO check if ok
def intensify(ind, A, n):  # ind - path, A - matrix, n - dimension
    def improve_path(path):
        g = -np.inf
        best_j = 0
        for j in range(path.size - 1):
            g_ = A[path[j], path[j + 1]] - A[path[j], path[-1]]
            if g_ > g:
                g = g_
                best_j = j
        if g <= 0:
            return None
        return np.hstack([path[:best_j], np.flipud(path[best_j:])])  # TODO ...?

    improvement = True
    ind_score = path_length(ind, A, n)
    while improvement:
        improvement = False
        for i in range(ind.size):
            res = improve_path(ind)
            if res is not None:
                res_score = path_length(res, A, n)
                if res_score < ind_score:
                    ind = res
                    ind_score = res_score
                    improvement = True
                    break
            ind = np.roll(ind, -1)
    return ind


### data visualization

def generate_image(V, E, A, n, out_file): # generate png image from graph file
    E_probs = get_edges_probs(V, E)
    E = set(E.keys())
    results = sorted([path_length(s, A, n) for s in V])  # ascending
    g_min = results[0]
    threshold = results[400]  # max number of nodes
    V = { s for s in V if path_length(s, A, n) <= threshold}  # s - path
    V_ = { s : (i, path_length(s, A, n)) for i, s in enumerate(V)}

    Not_Sinks = set()
    for s in V:  # TODO optimal?
        for (v, u) in E:
            if v == s:
                Not_Sinks.add(s)
                break

    V_c = np.zeros(len(V_))
    for s, (i, r) in V_.items():
        V_c[i] = (s, r)
    E_ = [(V_[s1][0], V_[s2][0]) for s1, s2 in E if s1 in V_ and s2 in V_]
    E_size = [5 * E_probs[s1, s2] for s1, s2 in E if s1 in V_ and s2 in V_]
    pos_glob = find_pos_glob(V_, E, g_min, A, n)

    g = igraph.Graph(directed=True)
    g.add_vertices(len(V_))
    g.add_edges(E_)

    visual_style = {}
    visual_style["layout"] = \
        g.layout_fruchterman_reingold(maxiter=5000)
    visual_style["vertex_color"] = ['red' if t[0] in pos_glob else 'blue'
                for t in V_c]
    visual_style["vertex_frame_color"] = \
        [visual_style["vertex_color"][i] if t[0] in Not_Sinks else 'black'
        for i, t in enumerate(V_c)]
    visual_style["vertex_frame_width"] = [2 for i in V_c]
    visual_style["vertex_size"] = [10 if t[0] in Not_Sinks else 20 for t in V_c]
    visual_style["edge_width"] = E_size
    visual_style["bbox"] = (0, 0, 1800, 1000)

    igraph.summary(g)
    image = igraph.plot(g, **visual_style)
    image.save(out_file + '.png')
    print("image ", out_file)


# automated subinstances graph image files generation
def gen_sub_images(filename, T, s):
    for t in range(T):
        sub_file = filename + "_" + str(s) + "_" + str(t)
        generate_image(sub_file)


# TODO probably IPython.display, function needed for Jupyter Notebook, needs checking
def print_subs(filename, T, s):
    for t in range(T):
        out_filename = filename + "_" + str(s) + "_" + str(t) + ".png"
        print(out_filename)
        img = Image(out_filename)
        display(img)


# graph parsing functions
def adjacency_list(V, E):
    E_ = {}
    for s1, s2 in E:
        if s1 in V and s2 in V:  # TODO why? any case it will be false?
            if s1 in E_:
                E_[s1].append(s2)
            else:
                E_[s1] = [s2]
    return E_


def dfs(v, V, E, g_min, prev, A, n):
    if path_length(v, A, n) == g_min:
        return True
    if v not in E:
        return False
    for u in E[v]:
        if u == prev:
            continue
        if dfs(u, V, E, g_min, v, A, n):
            return True
    return False


def find_pos_glob(V, E, g_min, A, n):
    E_ = adjacency_list(V, E)
    pos_glob = set()
    for v in V.keys():
        if dfs(v, V, E_, g_min, None, A, n):
            pos_glob.add(v)
    return pos_glob


def get_edges_probs(V, E):
    E_ = adjacency_list(V, set(E.keys()))  # key - (s1,s2)
    E_probs = {}
    for v, l in E_.items():
        s = sum([E[(v, neigh)] for neigh in l])
        for neigh in l:
            E_probs[(v, neigh)] = E[(v, neigh)] / s
    return E_probs  # probability of certain edge in all edges from certain node


### metrics

def generate_network_metrics(V, E, A, n, out_file):
    pass


def bfs(v, V, E):  #
    pass


### LON projection
def project_LON(V, E, A, n, out_file):
    pass


### helpers

def generate_subinstances(A, n, out_file, amount, nodes_removed):
    for t in range(amount):
        A_, n_, nodes_array = random_subinstance(A, n, n-nodes_removed)
        sub_name = out_file + "_" + str(nodes_removed) + "_" + str(t)
        save_TSP(A_, n_, nodes_array, sub_name)


def random_subinstance(A, n, n_):
    picked_v = np.sort(np.random.choice(range(n), n_, replace=False))
    return A[np.array(picked_v)[:,None], np.array(picked_v)], n_, picked_v  # only selected rows & columns


### main

def main_part1__generating_TSP_problems(tsp_filename, instances_foldername):  # step 1  # input bays29.tsp
    A, n = parse_TSP_from_file(tsp_filename)
    out_file = tsp_filename.split('.')[0]
    nodes_array = list(range(n))
    save_TSP(A, n, nodes_array, instances_foldername + "/" + out_file)  # nodes list, sub-instances have arrays
    generate_subinstances(A, n, instances_foldername + "/" + out_file, 9, 1)
    generate_subinstances(A, n, instances_foldername + "/" + out_file, 9, 2)
    generate_subinstances(A, n, instances_foldername + "/" + out_file, 10, 5)


def main_part2__generating_LONs(instances_foldername, lons_foldername, K):
    for filename in os.listdir(instances_foldername):
        A, n, nodes_array = load_TSP(instances_foldername + "/" + filename)
        out_file = filename.split('.')[0]

        # TODO paper [1] used 1000 Chained-LK runs and termination criterion: 10000 - decision needed
        L, E, best_path_length, best_path, successes, mean_iters = generate_LON(A, 100, 1000, K)
        save_LON(L, E, A, n, nodes_array, lons_foldername + "/" + out_file + ".g")
        save_metrics((best_path_length, best_path, successes, mean_iters), lons_foldername + "/" + out_file + ".metrics")
        print("LON ", out_file)


def main_part3__visualizing_LONs(lons_foldername, images_foldername):
    for filename in os.listdir(lons_foldername):
        L, E, A, n, nodes_array = load_LON(lons_foldername + "/" + filename)
        out_file = filename.split('.')[0]

        generate_image(L, E, A, n, images_foldername + "/" + out_file)


def main_part4__generating_metrics(lons_foldername, metrics_foldername):
    for filename in os.listdir(lons_foldername):
        L, E, A, n, nodes_array = load_LON(lons_foldername + "/" + filename)
        out_file = filename.split('.')[0]

        generate_network_metrics(L, E, A, n, metrics_foldername + "/" + out_file)


def main_part5__project_LON_on_subinstance(lons_foldername, projections_foldername):
    for filename in os.listdir(lons_foldername):
        L, E, A, n, nodes_array = load_LON(lons_foldername + "/" + filename)
        out_file = filename.split('.')[0]

        project_LON(L, E, A, n, projections_foldername + "/" + out_file)


# main_part1__generating_TSP_problems("bays29.tsp", "data")
# main_part2__generating_LONs("data", "lons", 1)
# main_part3__visualizing_LONs("lons", "images")
# main_part4__generating_metrics("lons", "metrics")
# main_part5__project_LON_on_subinstance("lons", "projections")
