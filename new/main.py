import numpy as np
import pickle
from multiprocessing import Pool
import time
import igraph
from IPython.display import Image, display
import os


### input/output, reading data

def save_TSP(A, n, file_name):
    with open(file_name + '.matrix', "wb") as fh:
        pickle.dump((A, n), fh)


def load_TSP(file_name):
    with open(file_name, "rb") as fh:
        (A, n) = pickle.load(fh)
        return A, n


def save_LON(V, E, A, n, file_name):
    # problem: A - adjacency matrix with weights, n - dimensions
    # landscape: V - optima vertices, E - escape edges
    with open(file_name, "wb") as fh:
        pickle.dump((V, E, A, n), fh)


def load_LON(file_name):
    with open(file_name, "rb") as fh:
        (V, E, A, n) = pickle.load(fh)
        return V, E, A, n


def parse_TSP_from_file(filename):
    with open(filename) as fh:
        data = fh.read().split("\n")  # lines of input file
    n = int(data[3].split()[1])  # dimensions (number of nodes)
    A = np.empty((n, n), dtype=int)  # adjacency matrix with weights

    for i in range(n):
        edges = map(int, data[i + 8].split())  # map(function, iterables), int(string, base)
        for j, v in enumerate(edges):
            A[i, j] = v

    return A, n


### fitness landscape

def generate_LON(A, n, runs_amount, termination_criterion):  # LON sampling (Algorithm 1 from paper)
    T = runs_amount  # number of Chained-LK runs
    I = termination_criterion  # termination criterion of run
    L = set()  # set of local optima
    E = {}  # escape edges

    pool = Pool()  # for parallel operations
    begin = time.time()
    for s_res, L_, E_ in pool.starmap(CLK, [(I, A) for t in range(T)]):  # starmap - map with multiple args # TODO
        L = L | L_  # set union  # TODO probable time losing place
        for k, v in E_.items():
            if k in E:
                E[k] += v
            else:
                E[k] = v

    print((time.time() - begin))
    return L, E


def CLK(I, A):  # Iterated local search Chained-LK
    # intensification stage: Lin-Kernighan local search
    # diversification stage: double bridge (4-exchange perturbation)

    def hashable_path(path):
        index = np.where(path == 0)[0][0]
        return tuple(np.roll(path, -index))

    n = A.shape[0]  # dimensions (number of nodes)
    s = np.random.permutation(n)  # random visiting nodes order
    s_res = path_length(s, A, n)  # path length for order s
    L = {hashable_path(s)}  # set of local optima (estimation generated by sampling)
    E = {}  # escape edges (obtained by double bridge and LK)
    i = 0
    while i < I:
        p = perturbate(s, 1)  # K double-bridge operations
        p = intensify(p, A, n)
        p_res = path_length(p, A, n)
        i += 1
        if p_res < s_res:
            L.add(hashable_path(p))  # adding hashable object
            e = (hashable_path(s), hashable_path(p))
            if e in E: # TODO why?
                E[e] += 1
            else:
                E[e] = 1
            s = p
            s_res = p_res
            i = 0
    return s_res, L, E


def path_length(p, A, n):
    s = 0.0
    for i in range(n):  # [0..n-1]
        s += A[p[i-1], p[i]]  # p[-1] - last element
    return s


def perturbate(nodes_order, K):  # K - kick strength (how many double-bridge operations are applied)
    def double_bridge(path):
        s = np.sort(np.random.choice(path.size, 3, replace=False))  # numpy.random.choice(int, sampleSize, copies?)
        return np.hstack([path[:s[0]], path[s[2]:], path[s[1]:s[2]], path[s[0]:s[1]]])

    nodes_order_ = nodes_order.copy()
    for i in range(K):
        nodes_order_ = double_bridge(nodes_order_)
    return nodes_order_


# LK
# TODO check if ok
def intensify(ind, A, n):  # ind - path, A - matrix, n - dimension
    def improve_path(path):
        g = -np.inf
        best_j = 0
        for j in range(path.size - 1):
            g_ = A[path[j], path[j + 1]] - A[path[j], path[-1]]
            if g_ > g:
                g = g_
                best_j = j
        if g <= 0:
            return None
        return np.hstack([path[:best_j], np.flipud(path[best_j:])])  # TODO ...?

    improvement = True
    ind_score = path_length(ind, A, n)
    while improvement:
        improvement = False
        for i in range(ind.size):
            res = improve_path(ind)
            if res is not None:
                res_score = path_length(res, A, n)
                if res_score < ind_score:
                    ind = res
                    ind_score = res_score
                    improvement = True
                    break
            ind = np.roll(ind, -1)
    return ind


### data visualization


def generate_image(V, E, A, n, out_file): # generate png image from graph file
    E_probs = get_edges_probs(V, E)
    E = set(E.keys())
    results = sorted([path_length(s, A, n) for s in V])  # ascending
    g_min = results[0]
    threshold = results[400]  # max number of nodes
    V = { s for s in V if path_length(s, A, n) <= threshold}  # s - path
    V_ = { s : (i, path_length(s, A, n)) for i, s in enumerate(V)}

    Not_Sinks = set()
    for s in V:  # TODO optimal?
        for (v, u) in E:
            if v == s:
                Not_Sinks.add(s)
                break

    V_c = np.zeros(len(V_))
    for s, (i, r) in V_.items():
        V_c[i] = (s, r)
    E_ = [(V_[s1][0], V_[s2][0]) for s1, s2 in E if s1 in V_ and s2 in V_]
    E_size = [5 * E_probs[s1, s2] for s1, s2 in E if s1 in V_ and s2 in V_]
    pos_glob = find_pos_glob(V_, E, g_min, A, n)

    g = igraph.Graph(directed=True)
    g.add_vertices(len(V_))
    g.add_edges(E_)

    visual_style = {}
    visual_style["layout"] = \
        g.layout_fruchterman_reingold(maxiter=5000)
    visual_style["vertex_color"] = ['red' if t[0] in pos_glob else 'blue'
                for t in V_c]
    visual_style["vertex_frame_color"] = \
        [visual_style["vertex_color"][i] if t[0] in Not_Sinks else 'black'
        for i, t in enumerate(V_c)]
    visual_style["vertex_frame_width"] = [2 for i in V_c]
    visual_style["vertex_size"] = [10 if t[0] in Not_Sinks else 20 for t in V_c]
    visual_style["edge_width"] = E_size
    visual_style["bbox"] = (0, 0, 1800, 1000)

    igraph.summary(g)
    image = igraph.plot(g, **visual_style)
    image.save('images/' + out_file + '.png')
    print("image ", out_file)







# automated subinstances graph image files generation
def gen_sub_images(filename, T, s):
    for t in range(T):
        sub_file = filename + "_" + str(s) + "_" + str(t)
        generate_image(sub_file)


# TODO probably IPython.display, needed for Jupyter Notebook
def print_subs(filename, T, s):
    for t in range(T):
        out_filename = filename + "_" + str(s) + "_" + str(t) + ".png"
        print(out_filename)
        img = Image(out_filename)  # TODO what's that?
        display(img)  # TODO what's that?



# graph parsing functions
def adjacency_list(V, E):
    E_ = {}
    for s1, s2 in E:
        if s1 in V and s2 in V:  # TODO why? any case it will be false?
            if s1 in E_:
                E_[s1].append(s2)
            else:
                E_[s1] = [s2]
    return E_


def dfs(v, V, E, g_min, prev, A, n):
    if path_length(v, A, n) == g_min:
        return True
    if v not in E:
        return False
    for u in E[v]:
        if u == prev:
            continue
        if dfs(u, V, E, g_min, v, A, n):
            return True
    return False


def find_pos_glob(V, E, g_min, A, n):
    E_ = adjacency_list(V, E)
    pos_glob = set()
    for v in V.keys():
        if dfs(v, V, E_, g_min, None, A, n):
            pos_glob.add(v)
    return pos_glob


def get_edges_probs(V, E):
    E_ = adjacency_list(V, set(E.keys()))  # key - (s1,s2)
    E_probs = {}
    for v, l in E_.items():
        s = sum([E[(v, neigh)] for neigh in l])
        for neigh in l:
            E_probs[(v, neigh)] = E[(v, neigh)] / s
    return E_probs  # probability of certain edge in all edges from certain node


def bfs(v, V, E):  #
    pass









### generate_metrics

def generate_metrics(V, E, A, n, out_file):
    pass






### helpers

def generate_subinstances(A, n, out_file, amount, nodes_removed):
    for t in range(amount):
        A_, n_ = random_subinstance(A, n, n-nodes_removed)
        sub_name = out_file + "_" + str(nodes_removed) + "_" + str(t)
        save_TSP(A_, n_, sub_name)


def random_subinstance(A, n, n_):  # TODO check, what's happening here?
    picked_v = np.sort(np.random.choice(range(n), n_, replace=False))
    return A[np.array(picked_v)[:,None], np.array(picked_v)], n_


### main

def main_part1__generating_TSP_problems(tsp_filename):  # step 1  # input bays29.tsp
    A, n = parse_TSP_from_file(tsp_filename)
    out_file = tsp_filename.split('.')[0]

    save_TSP(A, n, "data/" + out_file)
    generate_subinstances(A, n, "data/" + out_file, 9, 1)
    generate_subinstances(A, n, "data/" + out_file, 9, 2)
    generate_subinstances(A, n, "data/" + out_file, 10, 5)


def main_part2__generating_LONs():
    for filename in os.listdir('data'):
        A, n = load_TSP(filename)
        out_file = filename.split('.')[0]

        L, E = generate_LON(A, n, 1000, 10000)

        save_LON(L, E, A, n, "lons/" + out_file + ".g")
        print("LON ", out_file)


def main_part3__visualizing_LONs():
    for filename in os.listdir('lons'):
        L, E, A, n = load_LON(filename)
        out_file = filename.split('.')[0]

        generate_image(L, E, A, n, out_file)


def main_part4__generating_metrics():
    for filename in os.listdir('lons'):
        L, E, A, n = load_LON(filename)
        out_file = filename.split('.')[0]

        generate_metrics(L, E, A, n, out_file)


# generate_image("bays29")
# gen_sub_images("graphs/bays29", 9, 1)
# gen_sub_images("graphs/bays29", 9, 2)
# gen_sub_images("graphs/bays29", 10, 5)
# print_subs("graphs/bays29", 9, 1)
# print_subs("graphs/bays29", 9, 2)
# print_subs("graphs/bays29", 10, 5)
